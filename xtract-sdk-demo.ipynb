{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdaaabda",
   "metadata": {},
   "source": [
    "# Using Xtract to index research artifacts stored on Jetstream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea69d1",
   "metadata": {},
   "source": [
    "### This Xtract-Jetstream demo illustrates how to crawl, extract metadata from, and ingest metadata for any Globus Endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48986173",
   "metadata": {},
   "source": [
    "We begin by importing the Client and Endpoint classes from the xtract_sdk SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb620642",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xtract_sdk.client import XtractClient\n",
    "from xtract_sdk.endpoint import XtractEndpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c330aa26",
   "metadata": {},
   "source": [
    "## Step 1.a: Login\n",
    "### Creating an XtractClient object\n",
    "\n",
    "Here we create an XtractClient object to request tokens from Globus Auth. When fresh tokens are needed, users will authenticate with their Globus ID by following the directions in the STDOUT. Default auth scopes are as follow:\n",
    "\n",
    "* **openid**: provides username for identity.\n",
    "* **data_mdf**: FILL IN\n",
    "* **search**: interact with Globus Search\n",
    "* **petrel**: read or write data on Petrel. Not needed if no data going to Petrel.\n",
    "* **transfer**: needed to crawl the Globus endpoint and transfer metadata to its final location.\n",
    "* **dlhub**: FILL IN\n",
    "* **funcx_scope**: needed to orchestrate the metadata exraction at the given funcX endpoint.\n",
    "\n",
    "Additional auth scopes can be added with the `auth_scopes` argument.\n",
    "\n",
    "The following code block initializes all of the tokens by creating an `XtractClient` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf9cb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auths: {'openid': <globus_sdk.authorizers.refresh_token.RefreshTokenAuthorizer object at 0x7fb43decaf40>, 'https://auth.globus.org/scopes/facd7ccc-c5f4-42aa-916b-a0e270e2c2a9/all': <globus_sdk.authorizers.refresh_token.RefreshTokenAuthorizer object at 0x7fb43ded1610>, 'search_ingest': <globus_sdk.services.search.client.SearchClient object at 0x7fb43ded6fd0>, 'search': <globus_sdk.services.search.client.SearchClient object at 0x7fb43defb9d0>, 'petrel': <globus_sdk.authorizers.refresh_token.RefreshTokenAuthorizer object at 0x7fb43defb6d0>, 'transfer': <globus_sdk.services.transfer.client.TransferClient object at 0x7fb43e790130>}\n"
     ]
    }
   ],
   "source": [
    "xtr = XtractClient(auth_scopes=None, force_login=False)  \n",
    "print(f'Auths: {xtr.auths}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bd5e3",
   "metadata": {},
   "source": [
    "## Step 1.b: Defining endpoints\n",
    "### Creating an XtractEndpoint object\n",
    "\n",
    "Here we create an XtractEndpoint object to be used later in a crawl, etc.\n",
    "\n",
    "Required arguments are as follow:\n",
    "* **repo_type**: at this point, only Globus is accepted. GDrive and others to be implemented at a later date.\n",
    "* **globus_ep_id**: the source endpoint ID, at this point assumed to be a Globus ID (see previous bullet point)\n",
    "* **dirs**: directory paths for where the data resides\n",
    "* **grouper**: grouping strategy we want to use for grouping.\n",
    "\n",
    "The XtractEndpoint can also be given a `funcx_ep_id`.\n",
    "\n",
    "The following code block creates two `XtractEndpoint` objects which we will then be able to crawl on, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1131ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xep1 = XtractEndpoint(repo_type='globus',\n",
    "#                       globus_ep_id='f9959bd2-e98f-11eb-884c-aba19178789c',\n",
    "#                       funcx_ep_id='aaaa-0000-3333',\n",
    "#                       dirs=['/home/tskluzac/cord-19'], \n",
    "#                       grouper='file_is_group')\n",
    "\n",
    "# xep2 = XtractEndpoint(repo_type='globus',\n",
    "#                       globus_ep_id='f9959bd2-e98f-11eb-884c-aba19178789c',\n",
    "#                       dirs=['/home/tskluzac/cord-19'], \n",
    "#                       grouper='file_is_group')\n",
    "\n",
    "xep1 = XtractEndpoint(repo_type=\"GLOBUS\",\n",
    "                      globus_ep_id='4f99675c-ac1f-11ea-bee8-0e716405a293',\n",
    "                      dirs=['/cdiac/home/tskluzac/Downloads/'],\n",
    "                      grouper='file_is_group',\n",
    "                      local_mdata_path = \"/home/tskluzac/mdata\",\n",
    "                      remote_mdata_path = \"/home/my_metadata\")\n",
    "xep2 = XtractEndpoint(repo_type=\"Globus\",\n",
    "                      globus_ep_id='4f99675c-ac1f-11ea-bee8-0e716405a293',\n",
    "                      dirs=['/cdiac/home/tskluzac/Downloads/'],\n",
    "                      grouper='file_is_group',\n",
    "                      local_mdata_path = \"/home/tskluzac/mdata\",\n",
    "                      remote_mdata_path = \"/home/my_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a6624",
   "metadata": {},
   "source": [
    "## Step 2.a: Crawl\n",
    "Crawling, behind the scenes, will scan a Globus directory breadth-first (using globus_ls), first extracting physical metadata such as path, size, and extension. Next, since the *grouper* we selected is 'file_is_group', the crawler will simply create `n` single-file groups. \n",
    "\n",
    "The crawl is **non-blocking**, and the crawl_id here will be used to execute and monitor downstream extraction processes. \n",
    "\n",
    "The crawl ID for each endpoint will be stored in the XtractClient object as a list `self.crawl_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c71cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr.crawl([xep1, xep2])\n",
    "print(f'Crawl IDs: {xtr.crawl_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9fa1b",
   "metadata": {},
   "source": [
    "## Step 2.b: Crawl status\n",
    "We can get crawl status, seeing how many groups have been identified in the crawl. If `xtr.crawl()` has already been run, then `xtr.get_crawl_status()` will get the status of the IDs stored in `xtr.crawl_ids`. Otherwise, a list of `crawl_ids` may be given to `xtr.get_crawl_status()`.\n",
    "\n",
    "This will return a dictionary resembling: \n",
    "```\n",
    "{‘crawl_id’: ‘xxx’,\n",
    " ‘status’: ‘xxx’, \n",
    " ‘message’: “OK” if everything is fine otherwise describes error,\n",
    " ‘data’: {'bytes_crawled': xxx, ..., 'files_crawled': xxx}}\n",
    "```\n",
    "\n",
    "Note that measuring the total files yet to crawl is impossible, as the BFS may not have discovered all files yet, and Globus does not yet have a file counting feature for all directories and subdirectories. I.e., we know when we're done, but we do not know until we get there. \n",
    "\n",
    "**Warning:** it currently takes up to 30 seconds for a crawl to start. *Why?* Container warming time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "\n",
    "    crawl_statuses = xtr.get_crawl_status(crawl_ids=None)\n",
    "    for resp in crawl_statuses:\n",
    "        print(resp)\n",
    "\n",
    "    sub_statuses = [d['status'] for d in crawl_statuses]\n",
    "    if all(s == 'complete' for s in sub_statuses):\n",
    "        break\n",
    "\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de8d2a",
   "metadata": {},
   "source": [
    "## Step 2.c: Flushing crawl metadata\n",
    "\n",
    "After running a crawl, we can use `xtr.flush_crawl_metadata()` to return a list of all metadata from the crawl. \n",
    "\n",
    "Similarly with `.get_crawl_status()`, if `xtr.crawl()` has already been run, then `xtr.flush_crawl_metadata()` will get the status of the IDs stored in `xtr.crawl_ids`. Otherwise, a list of `crawl_ids` may be given to `xtr.flush_crawl_metadata()`.\n",
    "\n",
    "Flushing crawl metadata will return a dictionary resembling:\n",
    "```\n",
    "{\"crawl_id\": String,\n",
    " \"file_ls\": List,\n",
    " \"num_files\": Integer,\n",
    " \"queue_empty\": Boolean}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    print(xtr.flush_crawl_metadata(crawl_ids=None)\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa39ff",
   "metadata": {},
   "source": [
    "## Step 3: Xtract-ing\n",
    "\n",
    "Under construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6bbc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Response [404]>, <Response [404]>]\n"
     ]
    }
   ],
   "source": [
    "print(xtr.xtract([xep1,xep2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c07bde",
   "metadata": {},
   "source": [
    "## Step 3b: Getting Xtract status\n",
    "\n",
    "Under construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e31d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sp/wl4h63fd4j7czrdbxzpcwk7c0000gn/T/ipykernel_96058/1509267525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mxtract_statuses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xtract_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawl_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxtract_statuses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/11-13-hg/lib/python3.9/site-packages/xtract_sdk/client/xtract_client.py\u001b[0m in \u001b[0;36mget_xtract_status\u001b[0;34m(self, crawl_ids)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawl_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mxtract_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'crawl_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcid\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             payload.append({'xtract_status': json.loads(xtract_status.content)['status'],\n\u001b[0m\u001b[1;32m    177\u001b[0m                             'xtract_counters': json.loads(xtract_status.content)['counters']})\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/11-13-hg/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/11-13-hg/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/11-13-hg/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "\n",
    "    xtract_statuses = xtr.get_xtract_status(crawl_ids=None)\n",
    "    for resp in xtract_statuses:\n",
    "        print(resp)\n",
    "\n",
    "    sub_statuses = [d['status'] for d in xtract_statuses]\n",
    "    if all(s == 'complete' for s in sub_statuses):\n",
    "        break\n",
    "\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848749d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11-13-hg",
   "language": "python",
   "name": "11-13-hg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
