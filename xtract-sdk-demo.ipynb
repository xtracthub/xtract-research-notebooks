{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cbe52c2",
   "metadata": {},
   "source": [
    "# Xtract SDK v0.0.7a6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17dc540",
   "metadata": {},
   "source": [
    "## Login: Creating an XtractClient object\n",
    "First, we import the XtractClient class from the Xtract SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740cbf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xtract_sdk.client import XtractClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a113db4",
   "metadata": {},
   "source": [
    "Here we create an XtractClient object to request tokens from Globus Auth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ab783",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr = XtractClient(auth_scopes=[], force_login=False)\n",
    "print(f'Auths: {xtr.auths}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11401fdf",
   "metadata": {},
   "source": [
    "While additional auth scopes may be added with the `auth_scopes` argument, there are a number of \n",
    "default scopes automatically requested within the system. These are: \n",
    "\n",
    "* **openid**: provides username for identity.\n",
    "* **search**: interact with Globus Search\n",
    "* **petrel**: read or write data on Petrel. Not needed if no data going to Petrel.\n",
    "* **transfer**: needed to crawl the Globus endpoint and transfer metadata to its final location.\n",
    "* **funcx_scope**: needed to orchestrate the metadata exraction at the given funcX endpoint.\n",
    "\n",
    "Additional auth scopes can be added with the `auth_scopes` argument.\n",
    "\n",
    "When true, `force_login` makes you go through the full authorization flow again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f3587",
   "metadata": {},
   "source": [
    "## Defining endpoints: Creating an XtractEndpoint object\n",
    "Endpoints in Xtract are the computing fabric that enable us to move files and apply extractors to files. To this end, \n",
    "an Xtract endpoint is the combination of the following two software endpoints: \n",
    "* **Globus endpoints** [required] enable us to access all file system metadata about files stored on an endpoint, and enables us to transfer files between machines for more-efficient processing.\n",
    "* **FuncX endpoints** [optional] are capable of remotely receiving extraction functions that can be applied to files on the Globus endpoint. Note that the absence of a funcX endpoint on an Xtract endpoint means that a file must be transferred to an endpoint *with* a valid funcX endpoint in able to have its metadata extracted. \n",
    "\n",
    "In order to create an Xtract endpoint, we first import the XtractEndpoint class from the Xtract SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2238be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xtract_sdk.endpoint import XtractEndpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b492f054",
   "metadata": {},
   "source": [
    "Here we create an XtractEndpoint object to be used later in a crawl, xtraction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce50969",
   "metadata": {},
   "outputs": [],
   "source": [
    "xep1 = XtractEndpoint(repo_type=\"GLOBUS\",\n",
    "                      globus_ep_id='cb61bb16-5144-11ec-a6c6-9b4f84e67de8',\n",
    "                      funcx_ep_id='6b3a1745-5e0e-4c60-82db-0faac6cc246f',\n",
    "                      dirs=['/home/tskluzac/Documents/to-transfer-smaller'],\n",
    "                      local_mdata_path='/home/tskluzac/mdata',\n",
    "                      grouper='file_is_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8195c3",
   "metadata": {},
   "source": [
    "The arguments are as follow:\n",
    "* **repo_type**: (str) at this point, only Globus is accepted. Google Drive and others will be made available at a later date. \n",
    "* **globus_ep_id**: (uuid str) the Globus endpoint ID.\n",
    "* **funcx_ep_id**: (uuid str) optional funcX endpoint ID. \n",
    "* **dirs**: (list of str) directory paths on Globus endpoint for where the data reside.\n",
    "* **local_mdata_path** (str) directory path on Globus endpoint for where xtraction metadata should go.\n",
    "* **grouper**: (str) grouping strategy for files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c41daa",
   "metadata": {},
   "source": [
    "## Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d1161",
   "metadata": {},
   "source": [
    "`xtr.crawl([xep_1, ..., xep_n])`\n",
    "\n",
    "Where `[xep_1, ..., xep_n]` is a list of XtractEndpoint objects.\n",
    "\n",
    "The crawl ID for each endpoint will be stored in the XtractClient object as a list `xtr.crawl_ids`. Furthermore, each endpoint will be stored in the XtractClient object in a dictionary `cid_to_xep_map`, where each crawl id key maps to the corresponding endpoint as a value.\n",
    "\n",
    "Behind the scenes, this will scan a Globus directory breadth-first (using globus_ls), first extracting physical metadata such as path, size, and extension. Next, since the *grouper* we selected is 'file_is_group', the crawler will simply create `n` single-file groups. \n",
    "\n",
    "The crawl is **non-blocking**, and the crawl_id here will be used to execute and monitor downstream extraction processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr.crawl([xep1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281147e7",
   "metadata": {},
   "source": [
    "### Getting Crawl status\n",
    "\n",
    "`crawl_statuses = xtr.get_crawl_status(crawl_ids=None)`\n",
    "\n",
    "We can get crawl status, seeing how many groups have been identified in the crawl. If `xtr.crawl()` has already been run, then `xtr.get_crawl_status()` will get the status of the IDs stored in `xtr.crawl_ids`. Otherwise, a list of `crawl_ids` may be given to `xtr.get_crawl_status()`.\n",
    "\n",
    "This will return a dictionary resembling: \n",
    "```\n",
    "{‘crawl_id’: String,\n",
    " ‘status’: String, \n",
    " ‘message’: “OK” if everything is fine otherwise describes error,\n",
    " ‘data’: {'bytes_crawled': Integer, ..., 'files_crawled': Integer}}\n",
    "```\n",
    "\n",
    "Note that measuring the total files yet to crawl is impossible, as the BFS may not have discovered all files yet, and Globus does not yet have a file counting feature for all directories and subdirectories. I.e., we know when we're done, but we do not know until we get there. \n",
    "\n",
    "**Warning:** it currently takes up to 30 seconds for a crawl to start. *Why?* Container warming time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e75d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    \n",
    "    crawl_statuses = xtr.get_crawl_status(crawl_ids=None)\n",
    "    for resp in crawl_statuses:\n",
    "        print(resp)\n",
    "    \n",
    "    sub_statuses = [d['status'] for d in crawl_statuses]\n",
    "    if all(s == 'complete' for s in sub_statuses):\n",
    "        break\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24206017",
   "metadata": {},
   "source": [
    "### Crawl and wait\n",
    "\n",
    "For ease of testing, we've implemented a **crawl_and_wait** functionality, which will crawl the given endpoints and then print the crawl status of all given endpoints every two seconds until all have completed crawling. This can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef47c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr.crawl_and_wait([xep1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e039f8e",
   "metadata": {},
   "source": [
    "### Flushing Crawl metadata\n",
    "\n",
    "`xtr.flush_crawl_metadata(crawl_ids=None, first_n_files=100)`\n",
    "\n",
    "After running a crawl, we can use `xtr.flush_crawl_metadata()` to return a list of all metadata from the crawl. \n",
    "\n",
    "Similarly with `.get_crawl_status()`, if `xtr.crawl()` has already been run, then `xtr.flush_crawl_metadata()` will get the status of the IDs stored in `xtr.crawl_ids`. Otherwise, a list of `crawl_ids` may be given to `xtr.flush_crawl_metadata()`.\n",
    "\n",
    "Each time metadata is flushed, the number of files for which metadata is returned will be equal to `first_n_files`, and then that metadata will not be able to be flushed again.  \n",
    "\n",
    "Flushing crawl metadata will return a dictionary resembling:\n",
    "```\n",
    "{\"crawl_id\": String,\n",
    " \"file_ls\": List,\n",
    " \"num_files\": Integer,\n",
    " \"queue_empty\": Boolean}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4dd672",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    print(xtr.flush_crawl_metadata(crawl_ids=None, first_n_files=2))\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636b1308",
   "metadata": {},
   "source": [
    "## Xtract-ing\n",
    "\n",
    "### Registering containers for Xtraction\n",
    "\n",
    "`xtr.register_containers(endpoint, container_path)`\n",
    "\n",
    "In order to perform an xtraction, we must have the requisite containers for each extractor that is to be used. After creating client and endpoint instances, containers must be registered for each endpoint, using `.register_containers()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5845e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr.register_containers(xep1, container_path='/home/tskluzac/containers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4c3aa",
   "metadata": {},
   "source": [
    "Where the **container_path** (str) argument should be the path to the xtraction containers on the Globus endpoint.\n",
    "\n",
    "This can be executed regardless of **crawl** completion status.\n",
    "\n",
    "### Xtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9289351",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr.xtract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce24fe",
   "metadata": {},
   "source": [
    "The **crawl** method must have already been run, and an **xtract**ion will be run for each endpoint that was given to **crawl**. **xtract** will return the HTTP status response code, which should be 200.\n",
    "\n",
    "### Getting Xtract status\n",
    "`xtr.get_xtract_status()`\n",
    "\n",
    "The **xtract** method must have already been run, and this call will return a list of **xtract statuses**, one for each endpoint given to **crawl**.\n",
    "\n",
    "This will return a dictionary resembling:\n",
    "\n",
    "```\n",
    "{'xtract_status': String,\n",
    " 'xtract_counters': {'cumu_orch_enter': Integer, \n",
    "                     'cumu_pulled': Integer, \n",
    "                     'cumu_scheduled': Integer, \n",
    "                     'cumu_to_schedule': Integer, \n",
    "                     'flagged_unknown': Integer, \n",
    "                     'fx': {'failed': Integer, \n",
    "                            'pending': Integer, \n",
    "                            'success': Integer}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bddd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    \n",
    "    xtract_statuses = xtr.get_xtract_status()\n",
    "    for resp in xtract_statuses:\n",
    "        print(resp)\n",
    "    \n",
    "    sub_statuses = [d['xtract_status'] for d in xtract_statuses]\n",
    "    if all(s == 'COMPLETED' for s in sub_statuses):\n",
    "        break\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc2804",
   "metadata": {},
   "source": [
    "## Offload metadata\n",
    "\n",
    "`xtr.offload_metadata(dest_ep_id, dest_path=\"\", timeout=600, delete_source=False)`\n",
    "\n",
    "The **offload_metadata** method can be used to transfer files between two endpoints, and is included in this SDK for the purpose of transferring metadata from **xtract**ion. It takes the following arguments:\n",
    "* **dest_ep_id**: (str) the ID of the endpoint to which the files are being transferred.\n",
    "* **dest_path**: (optional str) the path on the destination endpoint where the files should go\n",
    "* **timeout**: (optional int, default 600) how long the transfer should wait until giving up if unsuccessful\n",
    "* **delete_source**: (optional boolean, default False) set to True if the source files should be deleted after metadata completion\n",
    "\n",
    "This method will transfer the metadata to a new folder (in the destination path, if supplied) which is named in the convention **YYYY-MM-DD-hh:mm:ss**. Calling the function will return the path to this folder on the destination endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr.offload_metadata(dest_ep_id='0caf6e8e-4974-11ec-a515-b537d6c07c1d',\n",
    "                     dest_path='Desktop/mdata/',\n",
    "                     delete_source=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "12-16-wf",
   "language": "python",
   "name": "12-16-wf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
