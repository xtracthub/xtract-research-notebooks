{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Xtract to index research artifacts stored on Jetstream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Xtract-Jetstream demo illustrates how to crawl, extract metadata from, and ingest metadata for any Globus Endpoint.\n",
    "\n",
    "#### We begin by importing important libraries. Of note, we use the `mdf_toolbox` library as a wrapper for Globus Auth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import mdf_toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Configuration\n",
    "\n",
    "#### Here we provide configuration details for our metadata extraction job, including specifications for both Globus and funcX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JETSTREAM: Globus endpoint and directory path AND funcX endpoint where the data reside\n",
    "source_ep_path_1 = \"/home/tskluzac/cord-19\" \n",
    "local_mdata_dir = \"/home/tskluzac/mdata\"\n",
    "source_ep_id = \"f9959bd2-e98f-11eb-884c-aba19178789c\"  # Globus ID for Jetstream instance 1\n",
    "funcx_ep_id = \"e1398319-0d0f-4188-909b-a978f6fc5621\"  # funcX ID for Jetstream instance 1\n",
    "\n",
    "\n",
    "\n",
    "# PETREL: Globus endpoint and file path at which we want to archive metadata documents\n",
    "mdata_ep_id = \"4f99675c-ac1f-11ea-bee8-0e716405a293\"  # Xtract Petrel EP: 4f...93\n",
    "remote_mdata_dir = \"/home/my_metadata\"\n",
    "\n",
    "# CRAWLER URL:\n",
    "# eb_crawl_url = \"http://xtractcrawler5-env.eba-akbhvznm.us-east-1.elasticbeanstalk.com\"\n",
    "eb_crawl_url = \"http://127.0.0.1:5000\"\n",
    "eb_extract_url = \"http://127.0.0.1:5000\"\n",
    "#eb_crawl_url = \"http://xtractservice2-env.eba-xh7cjv4i.us-east-1.elasticbeanstalk.com\"\n",
    "#eb_extract_url = \"http://xtractservice2-env.eba-xh7cjv4i.us-east-1.elasticbeanstalk.com\"\n",
    "\n",
    "# GROUPER: Grouping strategy we want to use for grouping. \"file_is_group\" means each file is a distinct data entity. \n",
    "grouper = \"file_is_group\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Login \n",
    "\n",
    "Here we use `mdf_toolbox` to request tokens from Globus Auth. When fresh tokens are needed, users will authenticate with their Globus ID by following the directions in the STDOUT. Notable auth scopes are as follows: \n",
    "\n",
    "* **openid**: provides username for identity.\n",
    "* **search**: interact with Globus Search\n",
    "* **petrel**: read or write data on Petrel. Not needed if no data going to Petrel.\n",
    "* **transfer**: needed to crawl the Globus endpoint and transfer metadata to its final location.\n",
    "* **funcx_scope**: needed to orchestrate the metadata exraction at the given funcX endpoint.\n",
    "\n",
    "The following code block initializes all of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Authenticating...\")\n",
    "funcx_scope = \"https://auth.globus.org/scopes/facd7ccc-c5f4-42aa-916b-a0e270e2c2a9/all\"\n",
    "search_all = \"urn:globus:auth:scope:search.api.globus.org:all\"\n",
    "auths = mdf_toolbox.login(\n",
    "    services=[\n",
    "        \"openid\",\n",
    "        \"data_mdf\",\n",
    "        \"search\",\n",
    "        \"petrel\",\n",
    "        \"transfer\",\n",
    "        search_all,\n",
    "        \"dlhub\",\n",
    "        funcx_scope,\n",
    "    ],\n",
    "    app_name=\"Foundry\",\n",
    "    make_clients=True,\n",
    "    no_browser=False,\n",
    "    no_local_server=False,\n",
    "    # force=True\n",
    ")\n",
    "print(\"Authentication successful!\")\n",
    "print(auths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Crawl\n",
    "Crawling, behind the scenes, will scan a Globus directory breadth-first (using globus_ls), first extracting physical metadata such as path, size, and extension. Next, since the *grouper* we selected is 'file_is_group', the crawler will simply create `n` single-file groups. \n",
    "\n",
    "The crawl is **non-blocking**, and the crawl_id here will be used to execute and monitor downstream extraction processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crawl_url = f'{eb_crawl_url}/crawl'\n",
    "print(f\"Crawl URL is : {crawl_url}\")\n",
    "\n",
    "first_ep_dict = {\n",
    "    'repo_type': 'GLOBUS',\n",
    "    'eid': source_ep_id,\n",
    "    'dir_paths': [source_ep_path_1], # Can add more than one path to this list. \n",
    "    'grouper': grouper\n",
    "}\n",
    "\n",
    "crawl_tokens = {'Transfer': auths['transfer'].authorizer.access_token, \n",
    "          'Authorization': f\"Bearer {auths['transfer'].authorizer.access_token}\", \n",
    "          'FuncX': auths[funcx_scope].access_token}  # , # 'Search': auths['search'].authorizer.access_token, \n",
    "          # OpenID': auths['openid'].access_token}\n",
    "\n",
    "crawl_req = requests.post(crawl_url, json={'endpoints': [first_ep_dict], 'tokens': crawl_tokens})\n",
    "print(crawl_req.content)\n",
    "crawl_id = json.loads(crawl_req.content)['crawl_id']\n",
    "print(f\"Crawl ID: {crawl_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get crawl status, seeing how many groups have been identified in the crawl. \n",
    "\n",
    "Note that measuring the total files yet to crawl is impossible, as the BFS may not have discovered all files yet, and Globus does not yet have a file counting feature for all directories and subdirectories. I.e., we know when we're done, but we do not know until we get there. \n",
    "\n",
    "**Warning:** it currently takes up to 30 seconds for a crawl to start. *Why?* Container warming time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True: \n",
    "    crawl_status = requests.get(f'{eb_crawl_url}/get_crawl_status', json={'crawl_id': crawl_id})\n",
    "    print(crawl_status)\n",
    "    crawl_content = json.loads(crawl_status.content)\n",
    "    print(f\"Crawl Status: {crawl_content}\")\n",
    "    \n",
    "    # Break the loop if we collected \n",
    "    if crawl_content['crawl_status'] == 'complete':\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a. You can directly flush the crawl metadata via REST \n",
    "\n",
    "#### Why? Downloading crawl metadata is useful for many file organization tasks, such as: \n",
    "- I want a list of all files on my file system\n",
    "- I want to know the total size (GB) of a folder\n",
    "- I want to tally files by extension\n",
    "\n",
    "#### Currently Foundry uses Xtract to create a list of all files in user-submitted folders. Check it out here: \n",
    "TODO: LINK TO FOUNDRY. \n",
    "\n",
    "**Caution**: if you flush the crawl metadata (3a), **you may not** extract metadata from them (3b). If you want to do both, you must launch two separate crawl jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     req = requests.get(f'{eb_url}/fetch_crawl_mdata', json={'crawl_id': crawl_id, 'n': 100})\n",
    "#     print(req.content)\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# # HERE WE WILL TEST CONFIGURING OUR ENDPOINT. \n",
    "# config_status = requests.post(f\"{eb_url}/configure_ep/{funcx_ep_id}\", json={'headers': fx_headers, \n",
    "#                                                                             'timeout': 25, \n",
    "#                                                                             'ep_name': 'tyler_test_ep_2', \n",
    "#                                                                             'globus_eid': '12345', \n",
    "#                                                                             'xtract_path':'/Users/tylerskluzacek/.xtract',\n",
    "#                                                                             'local_download_path': 'foobar',\n",
    "#                                                                             'local_mdata_path': '/Users/tylerskluzacek/Desktop/metadata'\n",
    "#                                                                      })\n",
    "# config_content = json.loads(config_status.content)\n",
    "# print(f\"Returned: {config_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Xtract\n",
    "\n",
    "Next we launch a non-blocking metadata extraction workflow that will automatically find all groups generated from our crawl_id, ship parsers to our endpoint as funcX, transfer the file (if necessary), and extract/send back metadata to the central Xtract service. This will just run constantly until the crawl is done and there are crawled groups left to extract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_headers = {'Authorization': f\"Bearer {auths[funcx_scope].access_token}\",\n",
    "             'Search': auths['search'].authorizer.access_token,\n",
    "             'Openid': auths['openid'].access_token}\n",
    "\n",
    "xtract = requests.post(f'{eb_extract_url}/extract', json={\n",
    "    'crawl_id': crawl_id, \n",
    "    'tokens': fx_headers, \n",
    "    'local_mdata_path': local_mdata_dir, \n",
    "    'remote_mdata_path': remote_mdata_dir})\n",
    "print(f\"Xtract response (should be 200): {xtract}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtract_status = requests.get(f'{eb_extract_url}/get_extract_status', json={'crawl_id': crawl_id})\n",
    "print(f\"Xtract Status: {json.loads(xtract_status.content)['status']}\")\n",
    "print(f\"Xtract Counters: {json.loads(xtract_status.content)['counters']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 (optional): Globus Search ingest\n",
    "\n",
    "#### In this step we create (and name) a Globus Search index for our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eb_extract_url = 'http://127.0.0.1:5000'\n",
    "\n",
    "search_index = \"ce2d9637-ad96-423f-99bc-935de889f640\"\n",
    "\n",
    "fx_headers = {'Authorization': f\"Bearer {auths[funcx_scope].access_token}\",\n",
    "             'Search': auths[search_all].authorizer.access_token,\n",
    "             'Openid': auths['openid'].access_token}\n",
    "\n",
    "search_info = {\n",
    "    'dataset_mdata': {'organizer':  'Tyler J. Skluzacek'},\n",
    "    'search_index_id': search_index,\n",
    "    'mdata_dir': local_mdata_dir,  \n",
    "    'tokens': fx_headers\n",
    "}\n",
    "\n",
    "resp = requests.post(f'{eb_extract_url}/ingest_search', json=search_info)\n",
    "print(resp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Metadata transfer (archive)\n",
    "\n",
    "#### Metadata, by default, are stored on the filesystem of the machine on which they were extracted. Here we can move them to a Globus endpoint of our choosing. \n",
    "\n",
    "Here I will push the metadata to ALCF's Petrel data store and opt not to DELETE them from Jetstream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {\"Transfer\": auths[\"petrel\"].access_token}\n",
    "\n",
    "while True:\n",
    "    xtract_status = requests.post(f'{eb_extract_url}/offload_mdata', json={\n",
    "        'crawl_id': crawl_id, \n",
    "        'tokens': crawl_tokens, \n",
    "        'source_ep': source_ep_id, \n",
    "        'mdata_ep': mdata_ep_id, \n",
    "        'delete_source': False})\n",
    "\n",
    "    response = json.loads(xtract_status.content)\n",
    "    print(response['status'])\n",
    "    if response['status'] == 'SUCCESS':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Let's query the index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMING SOON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11-13-hg",
   "language": "python",
   "name": "11-13-hg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
