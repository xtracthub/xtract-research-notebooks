{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee47bff7",
   "metadata": {},
   "source": [
    "# Deep learning model for multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337c0a46",
   "metadata": {},
   "source": [
    "Using scikit-learn to generate a multilabel classification dataset for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead8a6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example of a multi-label classification task\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "# define dataset\n",
    "X, y = make_multilabel_classification(n_samples=1000, n_features=10, n_classes=3, n_labels=2, random_state=1)\n",
    "# summarize dataset shape\n",
    "print(X.shape, y.shape)\n",
    "# summarize first few examples\n",
    "for i in range(10):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b245af73",
   "metadata": {},
   "source": [
    "We will define a Multilayer Perceptron (MLP) model for the multi-label classification task.\n",
    "\n",
    "Each sample has 10 inputs and three outputs; therefore, the network requires an input layer that expects 10 inputs specified via the “input_dim” argument in the first hidden layer and three nodes in the output layer.\n",
    "\n",
    "We will use the popular ReLU activation function in the hidden layer. The hidden layer has 20 nodes that author chose after some trial and error. We will fit the model using binary cross-entropy loss and the Adam version of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd52ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# define the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, # nodes in the hidden layer\n",
    "                    input_dim=n_inputs,\n",
    "                    kernel_initializer='he_uniform', \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(n_outputs, \n",
    "                    activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam')\n",
    "    return model\n",
    "\n",
    "get_model(10,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42895333",
   "metadata": {},
   "source": [
    "We will evaluate the MLP model on the multi-output regression task using repeated k-fold cross-validation with 10 folds and three repeats.\n",
    "\n",
    "The MLP model will predict the probability for each class label by default. This means it will predict three probabilities for each sample. These can be converted to crisp class labels by rounding the values to either 0 or 1.\n",
    "```\n",
    "# make a prediction on the test set\n",
    "yhat = model.predict(X_test)\n",
    "# round probabilities to class labels\n",
    "yhat = yhat.round()\n",
    "# calculate accuracy\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "```\n",
    "The scores are collected and can be summarized by reporting the mean and standard deviation across all repeats and cross-validation folds.\n",
    "\n",
    "The `evaluate_model()` function below takes the dataset, evaluates the model, and returns a list of evaluation scores, in this case, accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y):\n",
    "    results = list()\n",
    "    n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        # prepare data\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        # define model\n",
    "        model = get_model(n_inputs, n_outputs)\n",
    "        # fit model\n",
    "        model.fit(X_train, y_train, verbose=0, epochs=100)\n",
    "        # make a prediction on the test set\n",
    "        yhat = model.predict(X_test)\n",
    "#        print(yhat)\n",
    "        # round probabilities to class labels\n",
    "        yhat = yhat.round()\n",
    "        # calculate accuracy\n",
    "        acc = accuracy_score(y_test, yhat)\n",
    "        # store result\n",
    "        print('>%.3f' % acc)\n",
    "        results.append(acc)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662839b9",
   "metadata": {},
   "source": [
    "Here's a concrete example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceafd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X, y = make_multilabel_classification(n_samples=1000, n_features=10, n_classes=3, n_labels=2, random_state=1)\n",
    "# evaluate model\n",
    "results = evaluate_model(X, y)\n",
    "# summarize performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(results), std(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f743b0",
   "metadata": {},
   "source": [
    "# Using Scikit-Learn's automodel, plus playing around with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "X, Y = make_multilabel_classification(n_samples=3, random_state=0,\n",
    "                                      return_indicator=False)\n",
    "MultiLabelBinarizer().fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b68d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "predarray = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\n",
    "# print(f'X:{X}')\n",
    "# print(f'y:{y}')\n",
    "# print(numpy.corrcoef(y,predarray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "y = list(y)\n",
    "for i in range(len(y)):\n",
    "    if (y[i] == 0):\n",
    "        y[i] = (\"Iris-Setosa,\",)\n",
    "    elif (y[i] == 1):\n",
    "        y[i] = (\"Iris-Versicolour,\",)\n",
    "    elif (y[i] == 2):\n",
    "        y[i] = (\"Iris-Virginica,\",)\n",
    "\n",
    "ymlb = MultiLabelBinarizer()\n",
    "yfit = ymlb.fit_transform(y)\n",
    "\n",
    "def train_on_first_n_samples(n):\n",
    "    print(f'hello: {n}')\n",
    "    predarray = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X[:n], yfit[:n]).predict(X)\n",
    "    sum = 0\n",
    "    for i in range(len(yfit)):\n",
    "        if (np.array_equal(yfit[i],predarray[i])):\n",
    "            sum+=1\n",
    "    return(sum/len(y))\n",
    "\n",
    "def train_on_last_n_samples(n):\n",
    "    print(f'Trained on last {len(y)-n} samples:')\n",
    "    predarray = OneVsRestClassifier(LinearSVC(random_state=1)).fit(X[n:], yfit[n:]).predict(X)\n",
    "    sum = 0\n",
    "    for i in range(len(yfit)):\n",
    "        if (np.array_equal(yfit[i],predarray[i])):\n",
    "            sum+=1\n",
    "    return(sum/len(y))\n",
    "\n",
    "# for j in range(1,len(y)):\n",
    "#     print(train_on_last_n_samples(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44bcc3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "\n",
    "def plot_hyperplane(clf, min_x, max_x, linestyle, label):\n",
    "    # get the separating hyperplane\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(min_x - 5, max_x + 5)  # make sure the line is long enough\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "    pl.plot(xx, yy, linestyle, label=label)\n",
    "\n",
    "\n",
    "def plot_subfigure(X, Y, subplot, x_axis_trait, y_axis_trait):\n",
    "    #X = CCA(n_components=2).fit(X, Y).transform(X)\n",
    "\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "\n",
    "    classif = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "    classif.fit(X, Y)\n",
    "#    classif.predict()\n",
    "\n",
    "    pl.subplot(2, 3, subplot)\n",
    "\n",
    "    setosa = np.where(Y[:, 0])\n",
    "    versicolour = np.where(Y[:, 1])\n",
    "    virginica = np.where(Y[:, 2])\n",
    "\n",
    "    pl.scatter(X[setosa, x_axis_trait], X[setosa, y_axis_trait],\n",
    "               s=50, c='red', label='Iris Setosa')\n",
    "    pl.scatter(X[versicolour, x_axis_trait], X[versicolour, y_axis_trait],\n",
    "               s=50, c='green', label='Iris Versicolour')\n",
    "    pl.scatter(X[virginica, x_axis_trait], X[virginica, y_axis_trait],\n",
    "               s=50, c='blue', label='Iris Virginica')\n",
    "    \n",
    "\n",
    "#     plot_hyperplane(classif.estimators_[0], min_x, max_x, 'k--',\n",
    "#                     'Boundary\\nfor class 1')\n",
    "#     plot_hyperplane(classif.estimators_[1], min_x, max_x, 'k-.',\n",
    "#                     'Boundary\\nfor class 2')\n",
    "    pl.xticks(())\n",
    "    pl.yticks(())\n",
    "\n",
    "    pl.xlim(min_x - .5 * max_x, max_x + .5 * max_x)\n",
    "    pl.ylim(min_y - .5 * max_y, max_y + .5 * max_y)\n",
    "    pl.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "pl.figure(figsize=(14, 9))\n",
    "plot_subfigure(X, yfit, 1, 0, 1)\n",
    "plot_subfigure(X, yfit, 2, 0, 2)\n",
    "plot_subfigure(X, yfit, 3, 0, 3)\n",
    "plot_subfigure(X, yfit, 4, 1, 2)\n",
    "plot_subfigure(X, yfit, 5, 1, 3)\n",
    "plot_subfigure(X, yfit, 6, 2, 3)\n",
    "\n",
    "pl.subplots_adjust(.04, .02, .97, .94, .09, .2)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subfigure(X, Y, subplot, x_axis_trait, y_axis_trait):\n",
    "    #X = CCA(n_components=2).fit(X, Y).transform(X)\n",
    "\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "\n",
    "    classif = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "    classif.fit(X, Y)\n",
    "#    classif.predict()\n",
    "\n",
    "    pl.subplot(2, 3, subplot)\n",
    "\n",
    "    zero_class = np.where(Y[:, 0])\n",
    "    one_class = np.where(Y[:, 1])\n",
    "    #two_class = np.where(Y[:, 2])\n",
    "    #three_class = np.where(Y[:, 3])\n",
    "\n",
    "    pl.scatter(X[zero_class, x_axis_trait], X[zero_class, y_axis_trait],\n",
    "               s=40, c='red', label='zero class')\n",
    "    pl.scatter(X[one_class, x_axis_trait], X[one_class, y_axis_trait],\n",
    "               s=80, edgecolors='green',\n",
    "               facecolors='none', linewidths=2, label='one class')\n",
    "#     pl.scatter(X[two_class, x_axis_trait], X[two_class, y_axis_trait],\n",
    "#                s=160, edgecolors='b',\n",
    "#                facecolors='none', linewidths=2, label='two class')\n",
    "#     pl.scatter(X[three_class, x_axis_trait], X[three_class, y_axis_trait],\n",
    "#                s=320, edgecolors='orange',\n",
    "#                facecolors='none', linewidths=2, label='three class')\n",
    "\n",
    "#     plot_hyperplane(classif.estimators_[0], min_x, max_x, 'k--',\n",
    "#                     'Boundary\\nfor class 1')\n",
    "#     plot_hyperplane(classif.estimators_[1], min_x, max_x, 'k-.',\n",
    "#                     'Boundary\\nfor class 2')\n",
    "    pl.xticks(())\n",
    "    pl.yticks(())\n",
    "\n",
    "    pl.xlim(min_x - .5 * max_x, max_x + .5 * max_x)\n",
    "    pl.ylim(min_y - .5 * max_y, max_y + .5 * max_y)\n",
    "    pl.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "X, Y = make_multilabel_classification(n_samples=1000,\n",
    "                                      n_features=20,\n",
    "                                      n_classes=4,\n",
    "                                      n_labels=2,\n",
    "                                      allow_unlabeled=False,\n",
    "                                      random_state=0)\n",
    "\n",
    "def train_on_first_n_samples(n):\n",
    "    print(f'Trained on first {n} samples:')\n",
    "    predarray = OneVsRestClassifier(LinearSVC(random_state=1)).fit(X[:n], Y[:n]).predict(X[n:])\n",
    "    sum = 0\n",
    "    for i in range(len(predarray)):\n",
    "        if (np.array_equal(Y[i+n],predarray[i])):\n",
    "            sum+=1\n",
    "    return(sum/len(predarray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ef194",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_multilabel_classification(n_samples=1000,\n",
    "                                      n_features=2,\n",
    "                                      n_classes=4,\n",
    "                                      n_labels=2,\n",
    "                                      allow_unlabeled=False,\n",
    "                                      random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91055c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hyperplane(clf, min_x, max_x, linestyle, label):\n",
    "    # get the separating hyperplane\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(min_x - 5, max_x + 5)  # make sure the line is long enough\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "    pl.plot(xx, yy, linestyle, label=label)\n",
    "\n",
    "\n",
    "def plot_subfigure(X, Y, subplot, title, transform):\n",
    "    if transform == \"pca\":\n",
    "        X = PCA(n_components=2).fit_transform(X)\n",
    "    elif transform == \"cca\":\n",
    "        X = CCA(n_components=2).fit(X, Y).transform(X)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "\n",
    "    classif = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "    classif.fit(X, Y)\n",
    "\n",
    "    pl.subplot(2, 2, subplot)\n",
    "    pl.title(title)\n",
    "\n",
    "    zero_class = np.where(Y[:, 0])\n",
    "    one_class = np.where(Y[:, 1])\n",
    "    pl.scatter(X[:, 0], X[:, 1], s=40, c='gray')\n",
    "    pl.scatter(X[zero_class, 0], X[zero_class, 1], s=160, edgecolors='b',\n",
    "               facecolors='none', linewidths=2, label='Class 1')\n",
    "    pl.scatter(X[one_class, 0], X[one_class, 1], s=80, edgecolors='orange',\n",
    "               facecolors='none', linewidths=2, label='Class 2')\n",
    "\n",
    "    plot_hyperplane(classif.estimators_[0], min_x, max_x, 'k--',\n",
    "                    'Boundary\\nfor class 1')\n",
    "    plot_hyperplane(classif.estimators_[1], min_x, max_x, 'k-.',\n",
    "                    'Boundary\\nfor class 2')\n",
    "    pl.xticks(())\n",
    "    pl.yticks(())\n",
    "\n",
    "    pl.xlim(min_x - .5 * max_x, max_x + .5 * max_x)\n",
    "    pl.ylim(min_y - .5 * max_y, max_y + .5 * max_y)\n",
    "    if subplot == 2:\n",
    "        pl.xlabel('First principal component')\n",
    "        pl.ylabel('Second principal component')\n",
    "        pl.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "pl.figure(figsize=(8, 6))\n",
    "plot_subfigure(X, Y, 1, \"Without unlabeled samples + CCA\", \"cca\")\n",
    "\n",
    "pl.subplots_adjust(.04, .02, .97, .94, .09, .2)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337d865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "ml1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
