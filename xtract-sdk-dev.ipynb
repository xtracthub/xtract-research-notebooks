{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Xtract-SDK to extract metadata from files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xtract_sdk.client import XtractClient\n",
    "# print(f\"You are using xtract_sdk version: {xtract_sdk.__version__}\")  # TODO: get this working. \n",
    "\n",
    "source_ep_path_1 = \"/MDF/mdf_connect/prod/data/_test_einstein_9vpflvd_v1.1\"\n",
    "source_ep_id_1 = \"e38ee745-6d04-11e5-ba46-22000b92c6ec\"  # MDF at Petrel \n",
    "dest_ep_id_1 = \"af7bda53-6d04-11e5-ba46-22000b92c6ec\"  # where they will be extracted\n",
    "\n",
    "# Globus endpoint whatand file path at which we want to store metadata documents\n",
    "mdata_ep_id = \"5113667a-10b4-11ea-8a67-0e35e66293c2\"  \n",
    "mdata_path = \"/home/ubuntu/tskluzac\"\n",
    "\n",
    "# FuncX endpoint at which we want the metadata extraction to occur. Does NOT have to be same endpoint as the data.\n",
    "funcx_ep_id = \"b08b8612-cd87-47f4-b2ff-8c69c2c03d53\"\n",
    "\n",
    "# Grouping strategy\n",
    "grouper = \"file_is_group\"\n",
    "\n",
    "xc = XtractClient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auths)\n",
    "fx_scope = 'https://auth.globus.org/scopes/facd7ccc-c5f4-42aa-916b-a0e270e2c2a9/all'\n",
    "headers = {'Authorization': f\"Bearer {auths['petrel']}\", 'Transfer': auths['transfer'], 'FuncX': auths[fx_scope], 'Petrel': auths['petrel']}\n",
    "fx_headers = {'Authorization': f\"Bearer {auths[fx_scope].access_token}\",\n",
    "             'Search': auths['search'].authorizer.access_token,\n",
    "             'Openid': auths['openid'].access_token}\n",
    "print(\"\\n\" + str(fx_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Crawl\n",
    "Crawling, behind the scenes, will scan a Globus directory breadth-first (using globus_ls), first extracting physical metadata such as path, size, and extension. Next, since the *grouper* we selected is 'matio', the crawler will execute matio's `get_groups_by_postfix()` function on all file names in a directory in order to return groups for each of matio's parsers (besides *generic* and *noop*). \n",
    "\n",
    "The crawl will run as a non-blocking thread, and return a crawl_id that will be used extensively to track progress of our metadata extraction workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Adjust this to the Google Drive model!!!\n",
    "\n",
    "crawl_url = f'{eb_url}/crawl'\n",
    "print(f\"Crawl URL is : {crawl_url}\")\n",
    "\n",
    "first_ep_dict = {\n",
    "    'repo_type': 'GLOBUS',\n",
    "    'eid': source_ep_id,\n",
    "    'dir_paths': [source_ep_path_1],# , source_ep_path_2],\n",
    "    'grouper': grouper, \n",
    "    # 'prefetch_list':\n",
    "    #     [{'dest_ep_id': dest_ep_id, 'path': '/project2/chard/skluzacek/data_to_process'}]\n",
    "}\n",
    "\n",
    "tokens = {'Transfer': auths['transfer'].authorizer.access_token, \n",
    "          'Authorization': f\"Bearer {auths['petrel'].access_token}\", \n",
    "          'FuncX': auths[fx_scope].access_token}\n",
    "print(tokens)\n",
    "\n",
    "crawl_req = requests.post(crawl_url, json={'endpoints': [first_ep_dict], 'tokens': tokens})\n",
    "print(crawl_req.content)\n",
    "crawl_id = json.loads(crawl_req.content)['crawl_id']\n",
    "print(f\"Crawl ID: {crawl_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get crawl status, seeing how many groups have been identified in the crawl. \n",
    "\n",
    "Note that measuring the total files yet to crawl is impossible, as the BFS may not have discovered all files yet, and Globus does not yet have a file counting feature for all directories and subdirectories. I.e., we know when we're done, but we don't know until we get there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_id = '506c72c4-36dc-461f-8c69-4ed591f75b0a'\n",
    "# TODO: update the crawl status to query the DB (might require occasionally updating db)\n",
    "crawl_status = requests.get(f'{eb_url}/get_crawl_status', json={'crawl_id': crawl_id})\n",
    "print(crawl_status)\n",
    "crawl_content = json.loads(crawl_status.content)\n",
    "print(f\"Crawl Status: {crawl_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    req = requests.get(f'{eb_url}/fetch_crawl_mdata', json={'crawl_id': crawl_id, 'n': 100})\n",
    "    print(req.content)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_id\n",
    "# fetch_mdata = requests.get(f'{eb_url}/fetch_crawl_mdata', json={'crawl_id': crawl_id, 'n': 2})\n",
    "# print(fetch_mdata.content)\n",
    "# # fetch_content = json.loads(fetch_mdata.content)\n",
    "# print(f\"Crawl Status: {crawl_content}\")\n",
    "\n",
    "# tokens['Bearer']\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "\n",
    "\n",
    "# HERE WE WILL TEST CONFIGURING OUR ENDPOINT. \n",
    "config_status = requests.post(f\"{eb_url}/configure_ep/{funcx_ep_id}\", json={'headers': fx_headers, \n",
    "                                                                            'timeout': 25, \n",
    "                                                                            'ep_name': 'tyler_test_ep_2', \n",
    "                                                                            'globus_eid': '12345', \n",
    "                                                                            'xtract_path':'/Users/tylerskluzacek/.xtract',\n",
    "                                                                            'local_download_path': 'foobar',\n",
    "                                                                            'local_mdata_path': '/Users/tylerskluzacek/Desktop/metadata'\n",
    "                                                                            })\n",
    "config_content = json.loads(config_status.content)\n",
    "print(f\"Returned: {config_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Xtract\n",
    "\n",
    "Next we launch a non-blocking metadata extraction workflow that will automatically find all groups generated from our crawl_id, ship parsers to our endpoint as funcX, transfer the file (if necessary), and extract/send back metadata to the central Xtract service. This will just run constantly until the crawl is done and there are crawled groups left to extract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_id = \"246d0edf-2641-46f7-9700-3cc5a49c4890\"\n",
    "xtract = requests.post(f'{eb_url}/extract', json={'crawl_id': crawl_id,\n",
    "                                                  'repo_type': \"HTTPS\",\n",
    "                                                  'headers': json.dumps(headers),\n",
    "                                                  'funcx_eid': funcx_ep_id, \n",
    "                                                  'source_eid': source_ep_id,\n",
    "                                                  'dest_eid': dest_ep_id,\n",
    "                                                  'mdata_store_path': mdata_path, \n",
    "                                                  'data_prefetch_path': data_prefetch_path, \n",
    "                                                  'prefetch_remote': True})\n",
    "print(f\"Xtract response (should be 200): {xtract}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtract_status = requests.get(f'{eb_url}/get_extract_status', json={'crawl_id': crawl_id})\n",
    "xtract_content = json.loads(xtract_status.content)\n",
    "print(f\"Xtract Status: {xtract_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Access / Flush\n",
    "\n",
    "We might want to flush all new metadata blobs to a separate Globus endpoint. Here we initialize a results poller that creates a file of each metadata attribute to a folder at this path: `<mdata_path>/<crawl_id>/<group_id>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "while True: \n",
    "    poller = requests.post(f'{eb_url}/fetch_crawl_mdata', json={'crawl_id': crawl_id, 'Transfer': transfer_token, 'n': 100})\n",
    "    print(f'Flush Status: {poller}')\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plz)",
   "language": "python",
   "name": "plz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
